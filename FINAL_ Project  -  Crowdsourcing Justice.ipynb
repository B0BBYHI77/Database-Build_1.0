{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](CrowdsourcingJ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                              CROWDSOUCING JUSTICE \n",
    "##                             -by Rafael \"Rafa\" V. Baca, Esq.\n",
    "\n",
    "\n",
    "PREFACE: \n",
    "Historically, Immigration Law has been our nation's bellwether indicator for ensuring our inalienable, constitutionally-ensured, human rights and access to justice in our democratic society.  Current events have challenged these democratic freedoms that should never be taken for granted as immigrants are being turned away from our lands. Additionally, a growing number of poor and vulnerable populations in the U.S. are economically restricted from legal representation (1 lawyer for every 5000 legally harmed), and, thus, direct access to justice.\n",
    "\n",
    "MISSION STATEMENT: \n",
    "This project is an active continuation of my recent SXSW talk: \"Crowdsourcing Justice\" where the key idea is to apply an \"overarching\" software architectures and machine learning models that will provide continuous pro bono legal services by a revolving workforce of Attorneys and interested volunteers for vulnerable populations that typically do not presently share equal access to justice. Through my startup, Torchlight Legal, Pro Bono attorneys are given machine augmented litigation support, such as automated forms and legal research, to ensure existing cases are a success while increasing population of legal represented individuals.  Through a simple phone app, Torchlight Legal acts as the user interface between volunteer lawyers and supporting professionals in California, Texas, and New York and those vulnerable individuals who may be hesitant to go directly to law enforcement authorities or employers for help.\n",
    "\n",
    "INTITIAL PROJECTS [OPTIONAL - A MACHINE LEARNING USE CASE]: \n",
    "\n",
    "CASETEXT of San Francisco has provided use of the US Immigration (Administrative) Court’s body of legal cases so that Torchlight Legal will derive a dataset and create a database infrastructure for machine learning models to predict a court outcome based on prior rulings by a particular judge.  Accordingly, this project will demonstrate a \"proof-of-concept\" electronic database architecture for volunteer legal services intially within the states of California (One Justice) and Texas (RioGrande Legal Aid) to help pro bono clients in many categories of law beyond immigration law.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](OneJustice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJECT ARCHITECTURE \n",
    "## -- DIRECTED ACYCLIC GRAPHS, DAGs --\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1.)  FINAL PROJECT INFRASTRUCTURE \"PROOF-OF-CONCEPT\":\n",
    "\n",
    "[Optional - Technical Explination of Big Data Architecture]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](ArchiTLL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.)  ORIGINAL PROJECT INFRASTRUCTURE PROTOTYPES:\n",
    "\n",
    "[Optional - Technical Explination of Big Data Architecture]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical Specifications of Infrastructue - \n",
    "\n",
    "PROTOTYPE 2:  \n",
    "As the precursor to the above, \"proof-of-concept\" model, the below shows multiple datasources including streaming datasources.  After much research and individual solicitation, streaming datasources specifically directed toward the legal field were not in existance.  Notably, to account of streaming, Kensis, was added in to below architecture.  Staged data refers to a Relational Database Services provided by Postgres on AWS.  Accordingly, the tables of legal data would be queried via Postgres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](DAG01ver.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical Specifications of Infrastructue - \n",
    "\n",
    "PROTOTYPE 1:  \n",
    "The very first prototype shown below is an early architecture for a legal volunteer database.  In operation, a volunteer fills out a form on the Torchlight Legal website or app while on the internet layer via a local client, shown as a laptop.  The form shown below is based on a Google Docs template or equivilant technology.  On a continuing basis, data is retrieved from the fields of the volunteer forms by an EC2 instance on AWS.  The data is collected in S3 and then transformed to a resiliantly distributed dataset, RDD, via Postgress on the EC2 instance.  As the volunteer data is being collected, an email notification of receiving new volunteers is directly sent from google email hosting to the membership manager's inbox at Torchlight Legal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](DAG052of2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------- CODE 1 - Data to S3 ---------------\n",
    "The Following Code Transfers a local file to AWS S3.  The local file contains about 4000 recent law cases each with a legal ruling.  Each law case is formatted in XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto\n",
    "import os\n",
    "import pandas as pd\n",
    "import ssl\n",
    "import sys\n",
    "from boto.s3.connection import S3Connection\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    This pulls xml casefiles\n",
    "    1. Import credentals\n",
    "    2. Retrieve Casefiles\n",
    "    3. Loop Casefiles to Push to s3\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "# AWS Credentials\n",
    "S3Conn = S3Connection('MyAWSaccessKey','MyAWSsecretKey')\n",
    "greatbucket =S3Conn.get_bucket('bucketname')\n",
    "\n",
    "###  Accessing Desktop Folder\n",
    "path = '/Users/mycomputerName/Desktop/myFolder/'\n",
    "\n",
    "# UPLOAD BY LOOPING THRU BY INDIVIDUAL XML FILE\n",
    "for i in (os.listdir(path)):\n",
    "     file = greatbucket.new_key(i)\n",
    "     file.set_contents_from_filename(path+'/'+i)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## --------------- CODE 2  - S3 to RDS ----------------\n",
    "With an AWS EC2 instance executed by pyspark (Apache Spark's dialect of python), the following code transforms the case law data initially stored on S3 to become a RDD, resiliently distributated dataset.  Prior to this RDD transformation, tables (aspiring to be in the 3rd Normal form) where created on a Posgres (pgAdmin4) Relational Database Server (RDS). The RDS tables receive a parsed version of the S3 case law data that is executed by the following program:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TABLES are listed as follows:\n",
    "*******************************************\n",
    "TABLE 1   \n",
    "CASE\n",
    "Tag/col name \tdtype/postgres          KEY / Descrption\n",
    "docket      \t\t varhar    50\t\tPRIMARY KEY/CASE Docket \n",
    "reporter_caption\tvarchar 200\t\treporter citation (with yr and ct)\n",
    "date\t\t\t   Date\t\t\tDecision  Full Date  \n",
    "opinion_byline\ttext  \t\t\tPresiding Judge\n",
    "bold\t\t\ttext\t\t\tCase Holding/Ruling\n",
    "\n",
    "\n",
    "TABLE 2\n",
    "OPINION\n",
    "Tag/col name \tdtype/postgres          KEY / Descrption\n",
    "docket      \t\t varhar    50\t\tPRIMARY KEY/CASE Docket     len 50 \n",
    "opinion_text\t\ttext\t\t\tfull text of case\n",
    "(page_number\tsmallINT\t\tpp of opinion) - Optional\n",
    "footnote_number\tsmallINT\t\tfootnotes no. in case\n",
    "footnote_body\tvarchar\t\tfull text of footnotes\n",
    "\n",
    "TABLE 3\n",
    "CITED CASES\n",
    "Tag/col name \tdtype/postgres          KEY / Descrption\n",
    "docket      \t\t varhar    50\t\tPRIMARY KEY/CASE Docket     len 50 \n",
    "cross_reference\tvarchar   250\t\tcase and codified legal citations\n",
    "\n",
    "TABLE 4\n",
    "PLAYERS\n",
    "Tag/col name \tdtype/postgres          KEY / Descrption\n",
    "docket      \t\t varhar     50\t\tPRIMARY KEY/CASE Docket     len 50 \n",
    "panel\t\t\ttext\t\t\tJudges on Bench\n",
    "attorneys\t\ttext\t\t\tPlaintiff and Govt\t\n",
    "\n",
    "TABLE 5 (OPTIONAL)\n",
    "DISSENT\n",
    "Tag/col name \tdtype/postgres          KEY / Descrption\n",
    "docket      \t\t varhar    50\t\tPRIMARY KEY/CASE Docket     len 50 \n",
    "dissent_byline\ttext\t\t\tjudges writing byline\n",
    "dissent_text\t\ttext\t\t\tdissent text of opinion\n",
    "*******************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f3b3c4bed212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import boto\n",
    "from boto.s3.connection import S3Connection\n",
    "import os\n",
    "import ssl\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import psycopg2\n",
    "import string\n",
    "\n",
    "###########\n",
    "# AWS Credentials\n",
    "#S3Conn = S3Connection('MyAWSaccessKey','MyAWSsecretKey')\n",
    "\n",
    "# ------------ STEP ----------------#\n",
    "S3Conn = S3Connection('MyAWSaccessKey', 'MyAWSsecretKey',\n",
    "                      host='s3.amazonaws.com')\n",
    "greatbucket =S3Conn.get_bucket('bucketname')\n",
    "#greatbucket =S3Conn.get_bucket('testbucket')\n",
    "\n",
    "\n",
    "#spark context initialization & spark SQL initialize\n",
    "sc = SparkContext(\"local[*]\")\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# assigning soup as RDD\n",
    "path = \"s3://bucket/\"\n",
    "#path = \"s3://testbucket/\"\n",
    "\n",
    "#  --------STEP -------------#\n",
    "# create database connection\n",
    "dbConn = psycopg2.connect(**{\n",
    "         'dbname':'dbname',\n",
    "         'host':'host.usa-9.rds.amazonaws.com',\n",
    "         'user':'user',\n",
    "         'password':'pwd'\n",
    "         })\n",
    "cur = dbConn.cursor()\n",
    "#cur.execute('INSERT INTO \"CASE\" VALUES (%s, %s, now(), %s, %s)', (\"1\", \"2\", \"3\", \"4\") )\n",
    "#dbConn.commit()\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    This pulls xml casefiles on S3 bucketname bucket\n",
    "    1. Import credentals\n",
    "    2. Loop Casefiles to retrieve contents\n",
    "    3. while looping parsing and storing conents in dictionaries for corrsponding table\n",
    "    4.  generating a RDD on S3 based on dictionary values\n",
    "    '''\n",
    "    print( \"PATH: \", path )\n",
    "    partition( path )\n",
    "    #partition( \"foldername\" )\n",
    "    #pass\n",
    "\n",
    "# function created for list of files: os.listdir(path)\n",
    "# function to read xml files\n",
    "def partition(path):\n",
    "    counterDocket = 0\n",
    "    counterDuplicate = 0\n",
    "    counterSkip = 0\n",
    "    dockets = {}\n",
    "\n",
    "    #for i in (os.listdir(path)):\n",
    "    for key in greatbucket.list():\n",
    "            # if true or case with many names\n",
    "        if 1==1 or key.name == 'singlecaseName.xml':\n",
    "            #print(\"MYBUCKET: \",  key.name.encode('utf-8') )\n",
    "            print(\"MYFILE: \",  key.name )\n",
    "            #print(\"MYCONTENT: \",  key.get_contents_as_string() )\n",
    "\n",
    "            soup = BeautifulSoup( key.get_contents_as_string() , 'xml')\n",
    "\n",
    "            dataCASE = {\n",
    "                'docket' : '',\n",
    "                'reporter_caption' : '',\n",
    "                'date' : '',\n",
    "                'opinion_text' : '',\n",
    "                'find_order' : ''\n",
    "            }\n",
    "\n",
    "            cols = soup.find_all('docket')\n",
    "\n",
    "            dataCASE['docket'] = None\n",
    "            if len(cols) > 0:\n",
    "\n",
    "                # checking for spaces (on either side) on docket\n",
    "                # also works to specifically erase unwanted char- lft string\n",
    "                col0Trimmed = cols[0].get_text().lstrip()\n",
    "                if col0Trimmed.split(\" \") > 1:\n",
    "                    dataCASE['docket'] = col0Trimmed.split(\" \")[0]\n",
    "                else:\n",
    "                    dataCASE['docket'] = col0Trimmed\n",
    "\n",
    "                # checking for dots on docket\n",
    "                if dataCASE['docket'].split(\".\") > 1:\n",
    "                    dataCASE['docket'] = dataCASE['docket'].split(\".\")[0]\n",
    "\n",
    "                #print(\"MYDOCKET w true: \", len(cols), dataCASE['docket'], dataCASE['docket'] in dockets)\n",
    "\n",
    "            #print(\"SIZE: \", len(dockets) )\n",
    "            #if dataCASE['docket'] in dockets:\n",
    "                #print(\"IS DUPKICATE: \", dockets[dataCASE['docket']])\n",
    "# RAW DATA - VERY PROBLEMATIC BEGINNING WITH \"DOCKET\"\n",
    "            # if no docket, SKIP\n",
    "            if len(cols) > 0 and dataCASE['docket'] is not None and not dataCASE['docket'] in dockets:\n",
    "\n",
    "                #print(\"MYDOCKET IN: \", len(cols), dataCASE['docket'] )\n",
    "                # to keep track of dockets\n",
    "                counterDocket = counterDocket + 1\n",
    "                dockets[dataCASE['docket']] = dataCASE['docket']\n",
    "                #print(\"LENGTH XXX: \", len(dockets))\n",
    "\n",
    "                cols = soup.find_all('reporter_caption')\n",
    "                #print(\"MYREPORTER: \", len(cols), cols[0].get_text() )\n",
    "                dataCASE['reporter_caption'] = cols[0].get_text()\n",
    "\n",
    "                cols = soup.find_all('date')\n",
    "                #print(\"MYDATE: \", len(cols), cols[0].get_text() )\n",
    "                dataCASE['date'] = cols[0].get_text()\n",
    "\n",
    "                cols = soup.find_all('opinion_text')\n",
    "                if len(cols) > 0:\n",
    "                    #print(\"MYOPINION: \", len(cols), cols[0].get_text() )\n",
    "                    dataCASE['opinion_text'] = cols[0].get_text()\n",
    "                else:\n",
    "                    dataCASE['opinion_text'] = ''\n",
    "                    #print(\"MYOPINION: \", len(cols) )\n",
    "\n",
    "                # try to get order\n",
    "                bold = find_order( soup )\n",
    "                dataCASE['find_order'] = bold\n",
    "                #print(\"MYORDER: \", len(cols), bold )\n",
    "\n",
    "                # filter out binary data from find order / bold\n",
    "                #stripping non printable characters from a string\n",
    "                if dataCASE['find_order'] is not None:\n",
    "                    dataCASE['find_order'] = filter(lambda x: x in string.printable, dataCASE['find_order'])\n",
    "\n",
    "                # LOAD INTO DATABASE\n",
    "                cur.execute('INSERT INTO \"CASE\" ' +\n",
    "                    '(docket, reporter_caption, date, opinion_byline, bold) ' +\n",
    "                    'VALUES (%s, %s, %s, %s, %s)', (\n",
    "                        dataCASE['docket'],\n",
    "                        dataCASE['reporter_caption'],\n",
    "                        dataCASE['date'],\n",
    "                        dataCASE['opinion_text'],\n",
    "                        dataCASE['find_order']\n",
    "                    ))\n",
    "            else:\n",
    "                if len(cols) <= 0:\n",
    "                    counterSkip = counterSkip + 1\n",
    "                else:\n",
    "                    counterDuplicate = counterDuplicate + 1\n",
    "\n",
    "            print(\"Dockets Loded: \", counterDocket)\n",
    "            print(\"Dockets Duplicate : \", counterDuplicate)\n",
    "            print(\"Dockets Skip : \", counterSkip)\n",
    "\n",
    "    dbConn.commit()\n",
    "    print(\"Dockets Loded: \", counterDocket)\n",
    "    print(\"Dockets Duplicate : \", counterDuplicate)\n",
    "    print(\"Dockets Skip : \", counterSkip)\n",
    "\n",
    "#removing control characters/codes fm a string\n",
    "# aka nonprintable to\n",
    "# in this case it was <tag>[fn6]<tag>\n",
    "#https://github.com/nlplab/brat/blob/master/server/src/realmessage.py\n",
    "\n",
    "def remove_control_chars(s):\n",
    "    return control_char_re.sub('', s)\n",
    "\n",
    "# this function gets the Order for a Case\n",
    "def find_order(s):\n",
    "    result = None\n",
    "    for i in s.find_all('bold'):\n",
    "        #print(\"BOLD len: \", len(i))\n",
    "        #print(\"BOLD: \", str(i))\n",
    "        if str(i) == '<bold>ORDER:</bold>':\n",
    "            result = i.next_sibling\n",
    "            #print(\"GOT IN: \", len(i))\n",
    "            #print(\"RESULT BOLD: \", result)\n",
    "    #print(\"RESULT: \", result)\n",
    "    return result\n",
    "\n",
    "\n",
    "### TODO: THIS IS NOT BEING CALLED ###\n",
    "def opinion():\n",
    "    ##  Load opinion\n",
    "    dataOPINION = {\n",
    "        'opinion_text' : [],\n",
    "        'page_number' : [],\n",
    "        'footnote_number' : [],\n",
    "        'footnote_body' : [],\n",
    "    }\n",
    "\n",
    "    for soup in soups:\n",
    "        dataOPINION['docket'].append( cols[0].get_text() )\n",
    "        cols = soup.find_all('opinion_text')\n",
    "        dataOPINION['opinion_text'].append( cols[1].get_text() )\n",
    "        cols = soup.find_all('page_number')\n",
    "        dataOPINION['page_number'].append( cols[2].get_text() )\n",
    "        cols = soup.find_all('footnote_number')\n",
    "        dataOPINION['footnote_number'].append( cols[3].get_text() )\n",
    "        cols = soup.find_all('footnote_body')\n",
    "        dataOPINION['footnote_body'].append( cols[4].get_text() )\n",
    "    ##\n",
    "    dataCCASES = {\n",
    "        'cross_reference' : []\n",
    "    }\n",
    "\n",
    "    for soup in soups:\n",
    "        dataCCASES['docket'].append( cols[0].get_text() )\n",
    "        cols = soup.find_all('cross_reference')\n",
    "        dataCCASES['cross_reference'].append( cols[1].get_text() )\n",
    "    ##\n",
    "    dataPLAYERS = {\n",
    "        'panel' : [],\n",
    "        'attorneys' : []\n",
    "    }\n",
    "\n",
    "    for soup in soups:\n",
    "        dataPLAYERS['docket'].append( cols[0].get_text() )\n",
    "        cols = soup.find_all('panel')\n",
    "        dataPLAYERS['panel'].append( cols[1].get_text() )\n",
    "        cols = soup.find_all('attorneys')\n",
    "        dataPLAYERS['attorneys'].append( cols[2].get_text() )\n",
    "    ##\n",
    "    dataDISSENT = {\n",
    "        'dissent_byline' : [],\n",
    "        'dissent_text' : []\n",
    "    }\n",
    "\n",
    "    for soup in soups:\n",
    "        dataDISSENT['docket'].append( cols[0].get_text() )\n",
    "        cols = soup.find_all('dissent_byline')\n",
    "        dataDISSENT['dissent_byline'].append( cols[1].get_text() )\n",
    "        cols = soup.find_all('dissent_text')\n",
    "        dataDISSENT['dissent_text'].append( cols[2].get_text() )\n",
    "\n",
    "    #load data into database\n",
    "    #rdd.foreachPartition(soup)\n",
    "\n",
    "#caseData = pd.DataFrame( dataCASE )\n",
    "#caseData.to_csv(\"caseData.csv\")\n",
    "############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TABLE \"CASES\" BUILD OUTPUT:\n",
    "\n",
    "('Dockets Loded: ', 3298)\n",
    "\n",
    "('Dockets Duplicate : ', 487)\n",
    "\n",
    "('Dockets Skip : ', 181)\n",
    "\n",
    "('MYFILE: ', u'test/')\n",
    "\n",
    "('Dockets Loded: ', 3298)\n",
    "\n",
    "('Dockets Duplicate : ', 487)\n",
    "\n",
    "('Dockets Skip : ', 182)\n",
    "\n",
    "('MYFILE: ', u'test/singlelawcase.xml')\n",
    "\n",
    "('Dockets Loded: ', 3298)\n",
    "\n",
    "('Dockets Duplicate : ', 488)\n",
    "\n",
    "('Dockets Skip : ', 182)\n",
    "\n",
    "('MYFILE: ', u'test/singlelawcase.xml')\n",
    "\n",
    "('Dockets Loded: ', 3298)\n",
    "\n",
    "('Dockets Duplicate : ', 489)\n",
    "\n",
    "('Dockets Skip : ', 182)\n",
    "\n",
    "('Dockets Loded: ', 3298)\n",
    "\n",
    "('Dockets Duplicate : ', 489)\n",
    "\n",
    "('Dockets Skip : ', 182)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PYSPARK NOTES:  \n",
    "\n",
    "Personally, I found Zeppelin to not execute as desired during the debugging process.  Pyspark was subsequently exectuted directly from the terminal.  \n",
    "\n",
    "Here are some \n",
    "## helpful commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###########################\n",
    "# to connect to zeppelin\n",
    "ecListNumbersofDesiredEC2InstanceHere.compute1.amazonaws.com:8890\n",
    "\n",
    "Plus Amazon Login\n",
    "\n",
    "# copy data to ec2 instance\n",
    "scp –i NameofYourPemKey.pem 3DraftLegalSoup.py\n",
    "ec2-user@ecListNumberofDeseiredEC2InstanceDashes.compute-1.amazonaws.com :.\n",
    "\n",
    "# copy data to ec2 instance\n",
    "scp –r –i NameofYourPemKey.pem desiredBucketName ec2-user@ecListNuberofDesiredEC2InstanceDashes.compute-1.amazonaws.com :.    #Be sure not to forget \"period\" at end\n",
    "\n",
    "# to connect to the instance\n",
    "ssh –i NameofYourPEMKey.pem ec2user@ecListNumberNoDashofDesiredEC2Instance.compute1.amazonaws.com\n",
    "terminal looks like (you shoud be direct find the instance connection\n",
    "directly from the AWS ec2 Console:\n",
    "ComputerNames-MacBook-Pro-2:Desktop x$ ssh -i NAME.pem ec2-user@ec2-ListNumberDesiredInstanceDashes.compute-1.amazonaws.com\n",
    "\n",
    "# create aws config file with secret keys\n",
    "mkdir ~/.aws\n",
    "vi ~/.aws/config\n",
    "\n",
    "# add the following to the config file:\n",
    "[default]\n",
    "aws_secret_access_key = AWSSECRET ACCESS KEY\n",
    "aws_access_key_id = AWSACCESSKEY\n",
    "output = json\n",
    "region = SomeUSAplace-7\n",
    "\n",
    "# pick first xml file: lawcasename.xml\n",
    "ls –l | bucketName | head\n",
    "\n",
    "# Install pandas on EC2 instance\n",
    "sudo pip install –upgrade pip\n",
    "\n",
    "# logout and log back in\n",
    "sudo /usr/local/bin/pip install pandas\n",
    "\n",
    "# install pyspark and py4j\n",
    "sudo /usr/local/bin/pip install py4j\n",
    "\n",
    "################ Running On pyspark ############\n",
    "# USE sparksubmit\n",
    "spark-submit 3DraftLegalSoup.py\n",
    "\n",
    "[ec2-user@ip-9dashedNumbers ~]$ run python program\n",
    "\n",
    "-bash: run: \n",
    "\n",
    "[ec2-user@ip-9dashedNumbers ~]$ spark-submit 3DraftLegalSoup.py\n",
    "\n",
    "# to know if something failed use this:\n",
    "echo $?\n",
    "\n",
    "\n",
    "# to connect to postgresql from the terminal, use password: myDesiredPostgresServerPassword\n",
    "psql –h bucketName.awsaddress.usa-someRegion-4.rds.amazonaws.com –U bucketName\n",
    "\n",
    "\t\n",
    "# PLEASE find a terminal editor you like\n",
    "## --- To OPEN/&/CREATE FILES TYPE:\n",
    "Terminal editors abound, can be easily found with webrowser search:  Some editors tried - Nano, Emacs, & Vi.\n",
    "$   nano 3DraftLegalSoup.py\n",
    "$   emacs 3DraftLegalSoup.py\n",
    "$   vi 3DraftLegalSoup.py\n",
    "\n",
    "\n",
    "# how to edit the program\n",
    "\n",
    "NANO Editor – \n",
    "(COMMAND menu is given at the very bottom of the Editor in black\n",
    "BE sure to REMEMBER that ^X  the ^caret^ means “CONTROL” in MAC\n",
    "\n",
    "save the changes you've made, press Ctrl + O . \n",
    "EXIT nonModified -  nano, type Ctrl + X . \n",
    "EXIT from a modified file: it will ask you if you want to save it:  Y to Save, N not save - will then ask you for a filename.\n",
    "NANO-\n",
    "\n",
    "\n",
    "https://wiki.gentoo.org/wiki/Nano/Basics_Guide \n",
    "https://www.lifewire.com/beginners-guide-to-nano-editor-3859002 \n",
    "\n",
    "Posgres:\n",
    "USE TRUNCATE (Right CLICK on Desired Table – Truncate option) !!USE TRUNCATE AFTER EACH TIME YOU EDIT PROGRAM as NEW DATA NEEDS TO BE LOADED PURSUANT TO NEWEST CODE COMMANDS!!  TRUNCATE –quickly removes all rows from a set of tables. It has the same effect as an unqualified DELETE on each table, but because it does not actually scan the tables it is faster. Furthermore, it reclaims disk space immediately, rather than requiring a subsequent VACUUM operation. This is most useful on large tables.\n",
    "\n",
    "Right Click on Desired Table  – click PROPERTIES at Bottom  to review columns  -- to increase the size of databin\n",
    "\n",
    "RESTORE – Right Click on Desired Table – restores a backup database back on to S3.  http://stackoverflow.com/questions/2732474/restore-a-postgres-backup-file-using-the-command-line \n",
    "\n",
    "Posgres SQL:\n",
    "http://technobytz.com/most-useful-postgresql-commands.html \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- CODE 3 - RDS to QUERY & PUBLICATION -----\n",
    "With POSTGRES linked to the presently active AWS EC2 instance: \n",
    "\n",
    "1) A SQL qury was first applied to determine the number of unique Judicial rulings are contained in the present database. \n",
    "\n",
    "2) A python program was developed to operate on a csv file derived from the present database.  This python program counts the number of words provided in the judicial rulings and posts the result on an AWS WEBSITE HERE: https://s3.amazonaws.com/output6007/rulingWdTally.html .  The csv file was obtained through the dashboard provided by Postgres and, alternatively, through commands from the terminal line. The python program was implemented on a ipython notebook then transferred to a .py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) SQL QUERY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(**{\n",
    "         'dbname':'dbname',\n",
    "         'host':'dbname.awsaddress.usa-location-3.rds.amazonaws.com',\n",
    "         'user':'RDSserverUser',\n",
    "         'password':'password'\n",
    "         })\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT COUNT(DISTINCT judgeColumn) FROM \"TABLE\"')\n",
    "print(\"The total number of different Court Judgments is:\" , cur.fetchone()[0])\n",
    "\n",
    "#OUTPUT:    The total number of different Court Judgments is: 938"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) PYTHON PROGRAM: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for SQL Database access to Tables:\n",
    "import psycopg2\n",
    "\n",
    "#for pythonic activities:\n",
    "import pandas as pd\n",
    "from pandas import *\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# for AWS connections:\n",
    "import boto\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "from collections import Counter\n",
    "\n",
    "conn = psycopg2.connect(**{\n",
    "         'dbname':'dbname',\n",
    "         'host':'dbname.awsaddress.usa-location-3.rds.amazonaws.com',\n",
    "         'user':'RDSserverUser',\n",
    "         'password':'password'\n",
    "         })\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "\n",
    "#get rows with unique Judicial rulings from the database\n",
    "# bold column has at least one judicial ruling rendered in the case\n",
    "cur.execute('SELECT bold FROM \"CASE2\"')\n",
    "#cur.execute('SELECT COUNT(DISTINCT bold) FROM \"CASE2\"')\n",
    "\n",
    "#put SQL querried rows in a pandas dataframe\n",
    "rows = cur.fetchall()\n",
    "### Below is random exploration of the SQL to PYTHON (pandas) Experience\n",
    "#rowsDF = pd.DataFrame(rows)\n",
    "#print(rowsDF)\n",
    "#print(rows)\n",
    "#print(rows[3])\n",
    "##cur.execute('ROLLBACK')\n",
    "#rowsDF = pd.DataFrame(rows)\n",
    "#type(rows)\n",
    "#print(rowsDF[0])\n",
    "\n",
    "#print(rows) #a list\n",
    "str1 = str(rows) #a string\n",
    "#type(str1)\n",
    "every_word = str1.split(\" \")\n",
    "rule_wdcount = Counter(every_word)\n",
    "rulings = rule_wdcount.most_common(150)\n",
    "rule_df=pd.DataFrame(rulings, columns = ['WORD', 'FREQUENCY']) #pandas\n",
    "# print(rule_df)\n",
    "\n",
    "## Convert DF to HTML\n",
    "rule_df.to_html('rulingWdTally.html')\n",
    "#Counter(rowsDF[0])\n",
    "\n",
    "# Connecting to Desired AWS S3 bucket\n",
    "connS3 = S3Connection('AWSACCESSKEY', 'AWSSECRETKEY')  #need con\n",
    "postingBucket = connS3.get_bucket('output6007')\n",
    "\n",
    "# Tossing-up Output (Wd Ct for Judicial Rulings in dB) on the (AWS) web\n",
    "postingBucket.get_all_keys()\n",
    "file_key = postingBucket.new_key('rulingWdTally.html')\n",
    "\n",
    "file_key.content_type = 'text/html'\n",
    "file_key.set_contents_from_filename('rulingWdTally.html', policy='public-read')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the link for the output:\n",
    "https://s3.amazonaws.com/output6007/rulingWdTally.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](JUSTICEcomputer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LESSONS LEARNED & TO DO LIST:\n",
    "\n",
    "*  All 5 Goals of: Data Acquisition, Storage, Structure, Synthesizing, and Show have been accomplished within a very short timeframe.\n",
    "\n",
    "\n",
    "*  BeautifulSoup can also be a tool for parsing for XML files, the standard format for the legal support industry.\n",
    "\n",
    "\n",
    "*  Data Sets from third parties are a legal form of intellectual proerty, namely trade secrets.  Obaining permission to access such datasets from others, is nearly imposssible.  Assume that the process will take quite a long time, and set your general expectations toward never recieving any such dataset to keep ones expectations \"in-check\".\n",
    "\n",
    "   \n",
    "*  A common saying in the data science industry is that \"A DATASET PROJECT REQUIRES 80% CLEANING AND 20% APPLICATION\".  As these law cases were originally entered by hand by court clerks over past one hundred years onto a legal database, there were many typos, duplicates, and as well as omissions that requires meticulous, time consuming attention.\n",
    "   \n",
    "   \n",
    "* Continue to complete cleaning the immigration case data currently stored on AWS S3.  Torchlight Legal will then continue to develop machine learning models to assist volunteer attorneys with completing their legal research faster and thus taking on more underserved legal clients -- and winning more cases in court.\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](TorchlightL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## OUTCOMES -- 2.0 VERSION ARCHITECTURE [Optional]:\n",
    "\n",
    "* As more than one RDS database can be applied to a single EC2 instance (up to 1Tb), Torchlight Legal will work with OneJustice in San Francisco to build a database that stores information retrieved by legal service request applications provided by potential clients on the Torchlight Legal phone application.  Often, client intake applications are filled-out with paper forms on clipboards.\n",
    "\n",
    "\n",
    "* Torchlight Legal will work with \"developer-activists\" who want to impactfully take action to ensure that the benefits of our democracy will be enjoyed by all.  Accordingly, a \"Torchlight-Legal\" Github repo has been created to build out a front-end interface for Torchlight Legal so that any individual may potentially have direct access to legal representation.\n",
    "\n",
    "\n",
    "*  Presently, both the University of Texas - Austin and the Austin Tech Alliance have shown a great willingness to develop an open-source framework for creating simple legal intake forms for Torchlight Legal to implement in conjunction with those pro bono legal service entitles sanctioned by the State Bar of Texas.\n",
    "\n",
    "\n",
    "*  Innovation Law Labs (Portland, Oregon) is interested in adding documents that have proven highly successful in U.S. immigration courts for purposes of generating better machine learning models.  Ultimately, Innovation Law Labs wants to broaden its assistance to legally underserved populations, and Torchlight Legal intends to assist in those efforts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To help afflicted immigrants, the below recent photo shows's OneJustice's \"pop-up\" legal clinic established at San Francisco International Airport on the weekend after the 27 January 2017 travel ban was put in effect by the Trump Administration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](SFairportLawFirm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OPTIONAL - Start a Company]: \n",
    "\n",
    "Torchlight Legal has been legally incorporated.  Importantly, we are asking volunteer developers who want to take action to ensure that positive changes to the democratic system are for the good of all that share in this bounty of our great nation.  Torchlight Legal intends to create a USER INTERFACE for client intake (electronic forms and client information database development) for the phone app. so that vulnerable populations may receive legal services to thus expand and protect the human rights of all. \n",
    "\n",
    "https://github.com/Torchlight-Legal/Justice\n",
    "\n",
    "    \n",
    "## RESOURCES --- CROWDSOURCING JUSTICE:\n",
    "\n",
    "\n",
    "PODCAST - A panel discussion of how Tech may be impactfully applied to Justice System:\n",
    "* Crowdsourcing Justice - SXSW 2017 by SXSW on SoundCloud\n",
    "https://m.soundcloud.com/officialsxsw/crowdsourcing-justice-sxsw-2017 \n",
    "\n",
    "\n",
    "ARTICLES - Recent Activity in Justice Tech:\n",
    "* TEXAS BAR BLOG:\n",
    "    http://blog.texasbar.com/2017/03/articles/sxsw/sxsw-panel-how-attorneys-are-crowdsourcing-to-increase-access-to-justice/\n",
    "    \n",
    "    \n",
    "* AUSTIN TECH ALLIANCE:\n",
    "    https://medium.com/@AustinTechAll/guest-post-recapping-sxsws-crowdsourcing-justice-panel-40799888caa4\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All photos above used under Fair Use Doctrine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
